{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking\n",
    "\n",
    "Kevin J. Walchko\n",
    "\n",
    "created 28 Oct 2017\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [Object tracking with kalman filter](https://www.codeproject.com/Articles/865935/Object-Tracking-Kalman-Filter-with-Ease)\n",
    "- [Kalman filter for dummies](http://bilgin.esme.org/BitsAndBytes/KalmanFilterforDummies)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import cv2         # opencv itself\n",
    "import numpy as np # matrix manipulations\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common use for computer vision is detection and tracking of various objects through a scene. Typically there is a process flow of:\n",
    "\n",
    "1. clean up and calibrate image\n",
    "1. update\n",
    "    1. if *not* already tracking an object\n",
    "        1. then search the entire image and try to detect an object to track\n",
    "    1. if already tracking an object\n",
    "        1. based off the predicted location, detect the object within a window around the predicted location and update\n",
    "        1. if object not found, search entire image and update\n",
    "        1. if the object is not found, *esitmate* where the object should have been\n",
    "1. Using the new location, predict where the target will be in the future\n",
    "\n",
    "There are several issues which make tracking difficult:\n",
    "\n",
    "- target object initialization\n",
    "- occlusions which hide the target object\n",
    "- tracking multiple objects and difficulties that arise if they cross paths\n",
    "- scale, illumination and change of appearance\n",
    "- objects that have difficult and rapid motions\n",
    "\n",
    "Even though object tracking has been a problem for many years, it is still not solved despite the fact that there are many object trackers, even the ones built for special purposes, and those which want to solve the problem in general.\n",
    "\n",
    "Kalman filters, although they can be used for many other purposes, can be used for object tracking. They are especially convenient for objects which motion model is known, plus they incorporate some extra information in order to estimate the next object position more robustly. Unfortunately they are only useful for tracking single targets reliably.\n",
    "\n",
    "## Kalman Filter - Main Idea\n",
    "\n",
    "Let's assume that we have some kind of detector and that our detector is imperfect: it is prone to false positives, does not always detect objects, detects them imperfectly (does not provide exact position and scale) and its execution is costly. Let us also assume that we are tracking a single football player. After we detect the object, we want to leverage our information about the object in order to track it as robustly as we can. Our detector will give us just the location of the object so we have to use it as best as possible. In order to predict the next position of the player, we will need an object motion model (e.g. constant velocity motion, constant acceleration motion). Our detector is imperfect so there is noise in object locations, generally called measurement noise. Also, a selected motion model will not describe our player motion perfectly, so we also have noise regarding the model, called process noise. We want to estimate the next player position incorporating only the three parameters:\n",
    "\n",
    "- Object motion model\n",
    "- Measurement noise\n",
    "- Process noise\n",
    "\n",
    "So, we could efficiently re-detect it and hopefully cope with object occlusion. These next sections will show you how to do it.\n",
    "\n",
    "<img src=\"pics/cycle.png\" width=\"500px\">\n",
    "\n",
    "## Initial State\n",
    "\n",
    "We are going to first introduce the initial state and what we are trying to accomplish. The following figures show the initial state and in the same time introduce most relevant terms.\n",
    "\n",
    "<img src=\"pics/initial-1.png\" width=\"500px\">\n",
    "\n",
    "**Overview:** Using only estimates and the current state, we want to predict the next state. The second step (correction) includes a noisy measurement in order to apply a state update.\n",
    "\n",
    "<img src=\"pics/initial-2.png\" width=\"500px\">\n",
    "\n",
    "**Initial state type:** The green line at the top represents an object we’d like to track, with the blue X’s marking the object's true position. We want to model motion by using a constant velocity model. Therefore, the state will include the object position and velocity in both directions. The detector gives us the noisy object's position, so the position is our measurement.\n",
    "\n",
    "<img src=\"pics/initial-3.png\" width=\"500px\">\n",
    "\n",
    "**Initial state value:** In order to start the tracking process, we need to know the initial state x0|0 value. We also need to have the initial uncertainty which is expressed by Gaussian covariance matrix P0|0. The orange ellipse on the top represents the 2D Gaussian which describes the uncertainty in position. The initial matrix P0|0 is usually diagonal (assuming the components are not correlated), where the each component has its own uncertainty L - Gaussian sigma.\n",
    "\n",
    "Reading this section bear in mind:\n",
    "\n",
    "- x(t) - state vector\n",
    "- z(t) - measurement\n",
    "- P(t|t-1) - process covariance matrix\n",
    "\n",
    "## Predict\n",
    "\n",
    "Prediction is the first step which includes the next state (position and velocity) prediction as well as updating our uncertainty about the object state (increasing the uncertainty).\n",
    "\n",
    "<img src=\"pics/predict-1.png\" width=\"500px\">\n",
    "\n",
    "1a) State prediction: The first step is to predict the next state by using the motion model. The next state x(t|t-1) is obtained by multiplying the previous state by the state transition matrix - F. Q denotes process noise, which must follow normal distribution since we are working with Gaussians.\n",
    "\n",
    "<img src=\"pics/predict-2.png\" width=\"500px\">\n",
    "\n",
    "1b) Covariance prediction: The covariance update is done by multiplying the covariance matrix from the previous iteration by the state transition matrix F (motion model) and by adding the process noise Q which can be constant. The update covariance of our prediction is wider because we are less sure about our estimate.\n",
    "\n",
    "<img src=\"pics/predict-3.png\" width=\"500px\">\n",
    "\n",
    "* State transition matrix: The motion model must be represented by matrix F, therefore it must be linear. If the model is not linear the model must be linearized in some working point, which is used in the Extended Kalman Filter. The used model models the constant 2D velocity motion model where the position is updated as: p(t) = p(t-1) + v * p(t-1) where p denotes position and v velocity; the velocity remains constant. Velocity is marked as derivative of position in time. If we had done the prediction step multiple times, the estimated positions would follow the constant velocity model (blue dots). The covariance matrix would get wider each time (blue ellipse) as our uncertainty about the object position grows.\n",
    "\n",
    "Reading this section, bear in mind:\n",
    "\n",
    "- F - state transition matrix\n",
    "- Q(t) - process noise covariance\n",
    "\n",
    "## Correct\n",
    "\n",
    "After the noisy measurement has been obtained, the correction step begins. It incorporates a Kalman filter update which includes a state update and uncertainty update (decreasing the uncertainty).\n",
    "\n",
    "<img src=\"pics/correct-1.png\" width=\"500px\">\n",
    "\n",
    "* Measure: When (if) we receive a noisy measurement (in our case position obtained form a detector), the update process begins. The noisy measurement z(t) is modeled as a single Gaussian, where the noise is modeled as covariance matrix R(t)which is usually constant. The uncertainty of the measurement appears as golden ellipse.\n",
    "\n",
    "<img src=\"pics/correct-2.png\" width=\"500px\">\n",
    "\n",
    "2a) Measurement update: In order to calculate the predicted measurement needed for correction, we must select the measurement components from the state. A measurement has the same structure as a state or it just contains state parts; in our case, just the object position. Matrix H is the model selection matrix that when multiplied with state selects only elements that belong to a measurement.\n",
    "\n",
    "<img src=\"pics/correct-3.png\" width=\"500px\">\n",
    "\n",
    "* Residual: In order to make correction, we must know the prediction error. Therefore the residual, also known as innovation, is calculated, denoted by y(t). The residual is calculated by differencing the predicted measurement and obtained measurement - see the green and golden ellipse. Residual covariance S is calculated in a similar way.\n",
    "\n",
    "<img src=\"pics/correct-4.png\" width=\"500px\">\n",
    "\n",
    "* Kalman gain: Kalman gain K specifies how much we believe the prediction vs. how much we believe in the measurement. It is a product of predicted process covariance matrix P the observation model H and inverse residual covariance S. Let us study two extreme cases:\n",
    "\n",
    "- We are sure about measurements (we believe our detector)\n",
    "If this is the case, the measurement noise matrix R is very small which results the K decreases and the measurements are weighted more heavily than prediction.\n",
    "- We are sure about prediction (we believe in our motion model, no so much in the detector)\n",
    "If this is the case, the measurement noise matrix R is very large, which results in the K increases and the measurements are weighted much less than the prediction.\n",
    "\n",
    "<img src=\"pics/correct-5.png\" width=\"500px\">\n",
    "\n",
    "2b) Correct: The Kalman gain is now used to update the state xand covariance matrix P as shown on the figure. The result is the updated position which is denoted by the golden ellipse.\n",
    "\n",
    "<img src=\"pics/correct-6.png\" width=\"500px\">\n",
    "\n",
    "* Final step When the correction step is finished, the next step is, again, the prediction step. Those two steps are iteratively run in order to track an object as shown on the figure.\n",
    "\n",
    "Reading this section, you should know what is:\n",
    "\n",
    "- R(t) - measurement noise\n",
    "\n",
    "Now, when you know the basics, it is time for real samples, but first the brief introduction to the implementation.\n",
    "\n",
    "# Implementation\n",
    "\n",
    "A simple set of equations to track an object though a series of frames is:\n",
    "\n",
    "$$\n",
    "X_k = \\Phi X_{k-1} + N_{process} \\\\\n",
    "Z_k = H X_k + N_{measurement} \\\\\n",
    "X_k = \\begin{bmatrix}\n",
    "  x_k, y_k, v_{xk}, v_{yk}\n",
    "\\end{bmatrix}^T \\\\\n",
    "Z_k = \\begin{bmatrix}\n",
    "  z_x & z_y\n",
    "\\end{bmatrix}^T \\\\\n",
    "\\Phi = \\begin{bmatrix}\n",
    "  1 & 0 & \\delta t & 0 \\\\\n",
    "  0 & 1 & 0 & \\delta t \\\\\n",
    "  0 & 0 & 1 & 0 \\\\\n",
    "  0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "H = \\begin{bmatrix}\n",
    "  1 & 0 & 0 & 0 \\\\\n",
    "  0 & 1 & 0 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These kinematics are designed to say, an object should move smoothly from\n",
    "pixel to pixel and not randomly jump around on an image plane. We are only able to directly measure an object's position (x,y). The process noise is assumed to be $N(0,Q)$ and the measure noise is assumed to be $N(0,R)$ which are both gaussian.\n",
    "\n",
    "```python\n",
    "# setup kf\n",
    "kalman = cv2.KalmanFilter(4, 2, 0)  # state size, measurement size, control vector\n",
    "\n",
    "dt = 1.\n",
    "A = np.eye(4, dtype=np.float32)\n",
    "A[0, 2] =  dt\n",
    "A[1, 3] =  dt\n",
    "kalman.transitionMatrix = A\n",
    "\n",
    "H = np.array([[1., 0, 0, 0], [0, 1., 0, 0]], dtype=np.float32)\n",
    "kalman.measurementMatrix = H\n",
    "\n",
    "kalman.processNoiseCov = 1e-5 * np.eye(4, dtype=np.float32)\n",
    "kalman.measurementNoiseCov = 1e-1 * np.eye(2, dtype=np.float32)\n",
    "\n",
    "# initialize the kf state somehow\n",
    "kalman.statePre = np.array([x, y, vx, vy], dtype=np.float32)\n",
    "\n",
    "while(True):\n",
    "    measurement = grab_sample()  # measure something\n",
    "    kalman.correct(measurement)  # update kf with current measurement\n",
    "    prediction = kalman.predict()  # Computes a predicted state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blob Detection\n",
    "\n",
    "- [ref](https://www.learnopencv.com/blob-detection-using-opencv-python-c/)\n",
    "\n",
    "OpenCV has a simple blob detector built into it. The steps are:\n",
    "\n",
    "1. Convert the source image to binary images by applying thresholding with several thresholds from minThreshold (inclusive) to maxThreshold (exclusive) with distance thresholdStep between neighboring thresholds.\n",
    "1. Extract connected components from every binary image by findContours and calculate their centers.\n",
    "1. Group centers from several binary images by their coordinates. Close centers form one group that corresponds to one blob, which is controlled by the minDistBetweenBlobs parameter.\n",
    "1. From the groups, estimate final centers of blobs and their radiuses and return as locations and sizes of keypoints.\n",
    "\n",
    "The detector performs several filtrations of the discovered blobs. Available filtrations:\n",
    "\n",
    "- By color (**This seems to be broken**). This filter compares the intensity of a binary image at the center of a blob to blobColor. If they differ, the blob is filtered out. \n",
    "    - blobColor = 0 to extract dark blobs\n",
    "    - blobColor = 255 to extract light blobs.\n",
    "- By area. Extracted blobs have an area between minArea (inclusive) and maxArea (exclusive).\n",
    "- By circularity. Extracted blobs have circularity ($\\frac{4∗\\pi∗Area}{perimeter^2}$) between minCircularity (inclusive) and maxCircularity (exclusive).\n",
    "    - A circle has a circularity of 1\n",
    "- By ratio of the minimum inertia to maximum inertia. Extracted blobs have this ratio between minInertiaRatio (inclusive) and maxInertiaRatio (exclusive).\n",
    "- By convexity. Extracted blobs have convexity ($\\frac{area}{area_{blob-convex-hull}}$) between minConvexity (inclusive) and maxConvexity (exclusive).\n",
    "\n",
    "Default values of parameters are tuned to extract dark circular blobs. This, if you calculate a mask image, usually the object of interest is white, you will need to invert the image.\n",
    "\n",
    "```python\n",
    "params = cv2.SimpleBlobDetector_Params()\n",
    "\n",
    "# color - doesn't seem to work\n",
    "# params.blobColor = 255\n",
    "\n",
    "# Change thresholds - levels\n",
    "params.minThreshold = 0  # inclusive\n",
    "params.maxThreshold = 1  # exclusive\n",
    "params.thresholdStep = 1\n",
    "\n",
    "# Filter by Area.\n",
    "params.filterByArea = True\n",
    "params.minArea = 1500\n",
    "\n",
    "# Filter by Circularity - doesn't seem very precise\n",
    "params.filterByCircularity = True\n",
    "params.maxCircularity = 1.2\n",
    "params.minCircularity = .7\n",
    "\n",
    "# Filter by Convexity\n",
    "params.filterByConvexity = True\n",
    "params.minConvexity = 0.87\n",
    "\n",
    "# Filter by Inertia\n",
    "params.filterByInertia = True\n",
    "params.minInertiaRatio = 0.01\n",
    "\n",
    "detector = cv2.SimpleBlobDetector_create(params)\n",
    "keypoints = detector.detect(gray)\n",
    "im_with_keypoints = cv2.drawKeypoints(\n",
    "    frame,\n",
    "    keypoints,\n",
    "    None,  # output image\n",
    "    (0,0,255),\n",
    "    cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.imshow(im_with_keypoints)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
